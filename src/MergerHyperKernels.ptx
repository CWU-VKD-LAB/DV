//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35059454
// Cuda compilation tools, release 12.6, V12.6.85
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_75
.address_size 64

	// .globl	MergerHelper1
.global .align 1 .b8 __nv_static_43__74166147_21_MergerHyperKernels_cu_323b87d0__ZN52_INTERNAL_74166147_21_MergerHyperKernels_cu_323b87d04cuda3std6ranges3__45__cpo4swapE[1];
.extern .shared .align 4 .b8 seedBlockAttributes[];

.visible .entry MergerHelper1(
	.param .u64 MergerHelper1_param_0,
	.param .u64 MergerHelper1_param_1,
	.param .u64 MergerHelper1_param_2,
	.param .u64 MergerHelper1_param_3,
	.param .u64 MergerHelper1_param_4,
	.param .u64 MergerHelper1_param_5,
	.param .u32 MergerHelper1_param_6,
	.param .u64 MergerHelper1_param_7,
	.param .u32 MergerHelper1_param_8,
	.param .u32 MergerHelper1_param_9,
	.param .u64 MergerHelper1_param_10,
	.param .u64 MergerHelper1_param_11,
	.param .u64 MergerHelper1_param_12,
	.param .u32 MergerHelper1_param_13,
	.param .u64 MergerHelper1_param_14
)
{
	.reg .pred 	%p<97>;
	.reg .f32 	%f<62>;
	.reg .b32 	%r<285>;
	.reg .b64 	%rd<163>;


	ld.param.u64 	%rd50, [MergerHelper1_param_0];
	ld.param.u64 	%rd51, [MergerHelper1_param_1];
	ld.param.u64 	%rd52, [MergerHelper1_param_2];
	ld.param.u64 	%rd53, [MergerHelper1_param_3];
	ld.param.u64 	%rd54, [MergerHelper1_param_4];
	ld.param.u64 	%rd55, [MergerHelper1_param_5];
	ld.param.u32 	%r94, [MergerHelper1_param_6];
	ld.param.u64 	%rd47, [MergerHelper1_param_7];
	ld.param.u32 	%r95, [MergerHelper1_param_8];
	ld.param.u32 	%r96, [MergerHelper1_param_9];
	ld.param.u64 	%rd48, [MergerHelper1_param_10];
	ld.param.u32 	%r97, [MergerHelper1_param_13];
	ld.param.u64 	%rd49, [MergerHelper1_param_14];
	cvta.to.global.u64 	%rd1, %rd51;
	cvta.to.global.u64 	%rd2, %rd50;
	cvta.to.global.u64 	%rd3, %rd52;
	cvta.to.global.u64 	%rd4, %rd53;
	cvta.to.global.u64 	%rd5, %rd54;
	cvta.to.global.u64 	%rd6, %rd55;
	// begin inline asm
	mov.u32 %r98, %envreg2;
	// end inline asm
	cvt.u64.u32 	%rd56, %r98;
	// begin inline asm
	mov.u32 %r99, %envreg1;
	// end inline asm
	cvt.u64.u32 	%rd57, %r99;
	bfi.b64 	%rd7, %rd57, %rd56, 32, 32;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	mul.lo.s32 	%r100, %r4, %r94;
	cvt.s64.s32 	%rd8, %r100;
	setp.lt.s32 	%p1, %r96, 1;
	@%p1 bra 	$L__BB0_83;

	cvta.to.global.u64 	%rd158, %rd49;
	cvta.to.global.u64 	%rd157, %rd48;
	add.s64 	%rd11, %rd7, 4;
	mov.u32 	%r254, 0;
	mov.u32 	%r104, %nctaid.y;
	mov.u32 	%r105, %nctaid.x;
	mul.lo.s32 	%r106, %r105, %r104;
	mov.u32 	%r107, %nctaid.z;
	mul.lo.s32 	%r108, %r106, %r107;
	mov.u32 	%r109, %ctaid.y;
	add.s32 	%r110, %r2, %r109;
	mov.u32 	%r111, %ctaid.z;
	neg.s32 	%r112, %r111;
	setp.eq.s32 	%p2, %r110, %r112;
	mov.u32 	%r113, -2147483647;
	sub.s32 	%r114, %r113, %r108;
	selp.b32 	%r7, %r114, 1, %p2;
	not.b32 	%r115, %r4;
	add.s32 	%r116, %r115, %r96;
	div.u32 	%r9, %r116, %r97;
	add.s32 	%r117, %r9, 1;
	and.b32  	%r10, %r94, 3;
	sub.s32 	%r11, %r94, %r10;
	and.b32  	%r12, %r117, 3;
	mul.wide.s32 	%rd58, %r4, 4;
	add.s64 	%rd12, %rd6, %rd58;
	mul.wide.s32 	%rd17, %r97, 4;
	add.s64 	%rd13, %rd12, %rd17;
	add.s32 	%r13, %r4, %r97;
	add.s32 	%r14, %r13, %r97;
	add.s64 	%rd14, %rd13, %rd17;
	add.s32 	%r15, %r14, %r97;
	shl.b64 	%rd59, %rd8, 2;
	add.s64 	%rd60, %rd59, 8;
	add.s64 	%rd15, %rd3, %rd60;
	add.s64 	%rd16, %rd4, %rd60;
	shl.b32 	%r118, %r94, 2;
	mov.u32 	%r119, seedBlockAttributes;
	add.s32 	%r120, %r119, %r118;
	add.s32 	%r16, %r120, 8;
	cvta.to.global.u64 	%rd18, %rd47;

$L__BB0_2:
	mov.u64 	%rd20, %rd158;
	setp.ne.s32 	%p3, %r254, 0;
	selp.b64 	%rd158, %rd157, %rd20, %p3;
	selp.b64 	%rd157, %rd20, %rd157, %p3;
	mul.wide.s32 	%rd61, %r254, 4;
	add.s64 	%rd62, %rd157, %rd61;
	ld.global.s32 	%rd23, [%rd62];
	setp.ge.s32 	%p4, %r3, %r94;
	@%p4 bra 	$L__BB0_5;

	cvt.u32.u64 	%r121, %rd23;
	mul.lo.s32 	%r18, %r121, %r94;
	mov.u32 	%r255, %r3;

$L__BB0_4:
	mov.u32 	%r236, %ntid.x;
	shl.b32 	%r235, %r94, 2;
	add.s32 	%r122, %r255, %r18;
	mul.wide.s32 	%rd63, %r122, 4;
	add.s64 	%rd64, %rd2, %rd63;
	ld.global.f32 	%f2, [%rd64];
	shl.b32 	%r123, %r255, 2;
	mov.u32 	%r124, seedBlockAttributes;
	add.s32 	%r125, %r124, %r123;
	st.shared.f32 	[%r125], %f2;
	add.s64 	%rd65, %rd1, %rd63;
	ld.global.f32 	%f3, [%rd65];
	add.s32 	%r127, %r125, %r235;
	st.shared.f32 	[%r127], %f3;
	add.s32 	%r255, %r255, %r236;
	setp.lt.s32 	%p5, %r255, %r94;
	@%p5 bra 	$L__BB0_4;

$L__BB0_5:
	setp.ne.s64 	%p6, %rd7, 0;
	@%p6 bra 	$L__BB0_7;

	// begin inline asm
	trap;
	// end inline asm

$L__BB0_7:
	mov.u32 	%r234, %tid.z;
	neg.s32 	%r233, %r234;
	mov.u32 	%r232, %tid.y;
	add.s32 	%r231, %r3, %r232;
	setp.ne.s32 	%p7, %r231, %r233;
	barrier.sync 	0;
	@%p7 bra 	$L__BB0_10;

	// begin inline asm
	atom.add.release.gpu.u32 %r128,[%rd11],%r7;
	// end inline asm

$L__BB0_9:
	// begin inline asm
	ld.acquire.gpu.u32 %r130,[%rd11];
	// end inline asm
	xor.b32  	%r131, %r130, %r128;
	setp.gt.s32 	%p8, %r131, -1;
	@%p8 bra 	$L__BB0_9;

$L__BB0_10:
	setp.ge.s32 	%p9, %r4, %r96;
	barrier.sync 	0;
	@%p9 bra 	$L__BB0_43;

	setp.gt.s32 	%p10, %r94, 0;
	@%p10 bra 	$L__BB0_17;
	bra.uni 	$L__BB0_12;

$L__BB0_17:
	mov.u32 	%r257, %r4;

$L__BB0_18:
	cvt.u32.u64 	%r136, %rd23;
	setp.eq.s32 	%p16, %r257, %r136;
	@%p16 bra 	$L__BB0_42;

	mul.wide.s32 	%rd73, %r257, 4;
	add.s64 	%rd74, %rd5, %rd73;
	ld.global.u32 	%r137, [%rd74];
	setp.eq.s32 	%p17, %r137, -1;
	@%p17 bra 	$L__BB0_42;

	add.s32 	%r240, %r94, -1;
	setp.lt.u32 	%p18, %r240, 3;
	mul.lo.s32 	%r25, %r257, %r94;
	mov.u32 	%r262, 0;
	@%p18 bra 	$L__BB0_23;

	mov.u32 	%r262, 0;
	mov.u32 	%r258, %r119;
	mov.u32 	%r259, %r16;
	mov.u64 	%rd159, %rd16;
	mov.u64 	%rd160, %rd15;
	mov.u64 	%rd161, %rd2;
	mov.u64 	%rd162, %rd1;
	mov.u32 	%r261, %r11;

$L__BB0_22:
	mul.lo.s32 	%r241, %r257, %r94;
	mul.wide.s32 	%rd156, %r241, 4;
	add.s64 	%rd75, %rd162, %rd156;
	ld.global.f32 	%f4, [%rd75];
	ld.shared.f32 	%f5, [%r259+-8];
	setp.gt.f32 	%p19, %f5, %f4;
	selp.f32 	%f6, %f5, %f4, %p19;
	st.global.f32 	[%rd159+-8], %f6;
	add.s64 	%rd76, %rd161, %rd156;
	ld.global.f32 	%f7, [%rd76];
	ld.shared.f32 	%f8, [%r258];
	setp.gt.f32 	%p20, %f8, %f7;
	selp.f32 	%f9, %f7, %f8, %p20;
	st.global.f32 	[%rd160+-8], %f9;
	ld.global.f32 	%f10, [%rd75+4];
	ld.shared.f32 	%f11, [%r259+-4];
	setp.gt.f32 	%p21, %f11, %f10;
	selp.f32 	%f12, %f11, %f10, %p21;
	st.global.f32 	[%rd159+-4], %f12;
	ld.global.f32 	%f13, [%rd76+4];
	ld.shared.f32 	%f14, [%r258+4];
	setp.gt.f32 	%p22, %f14, %f13;
	selp.f32 	%f15, %f13, %f14, %p22;
	st.global.f32 	[%rd160+-4], %f15;
	ld.global.f32 	%f16, [%rd75+8];
	ld.shared.f32 	%f17, [%r259];
	setp.gt.f32 	%p23, %f17, %f16;
	selp.f32 	%f18, %f17, %f16, %p23;
	st.global.f32 	[%rd159], %f18;
	ld.global.f32 	%f19, [%rd76+8];
	ld.shared.f32 	%f20, [%r258+8];
	setp.gt.f32 	%p24, %f20, %f19;
	selp.f32 	%f21, %f19, %f20, %p24;
	st.global.f32 	[%rd160], %f21;
	ld.global.f32 	%f22, [%rd75+12];
	ld.shared.f32 	%f23, [%r259+4];
	setp.gt.f32 	%p25, %f23, %f22;
	selp.f32 	%f24, %f23, %f22, %p25;
	st.global.f32 	[%rd159+4], %f24;
	ld.global.f32 	%f25, [%rd76+12];
	ld.shared.f32 	%f26, [%r258+12];
	setp.gt.f32 	%p26, %f26, %f25;
	selp.f32 	%f27, %f25, %f26, %p26;
	st.global.f32 	[%rd160+4], %f27;
	add.s32 	%r262, %r262, 4;
	add.s64 	%rd162, %rd162, 16;
	add.s64 	%rd161, %rd161, 16;
	add.s64 	%rd160, %rd160, 16;
	add.s64 	%rd159, %rd159, 16;
	add.s32 	%r259, %r259, 16;
	add.s32 	%r258, %r258, 16;
	add.s32 	%r261, %r261, -4;
	setp.ne.s32 	%p27, %r261, 0;
	@%p27 bra 	$L__BB0_22;

$L__BB0_23:
	setp.eq.s32 	%p28, %r10, 0;
	@%p28 bra 	$L__BB0_27;

	setp.eq.s32 	%p29, %r10, 1;
	add.s32 	%r141, %r262, %r94;
	shl.b32 	%r142, %r141, 2;
	add.s32 	%r36, %r119, %r142;
	add.s32 	%r144, %r262, %r25;
	mul.wide.s32 	%rd77, %r144, 4;
	add.s64 	%rd36, %rd1, %rd77;
	ld.global.f32 	%f28, [%rd36];
	ld.shared.f32 	%f29, [%r36];
	setp.gt.f32 	%p30, %f29, %f28;
	selp.f32 	%f30, %f29, %f28, %p30;
	cvt.s64.s32 	%rd78, %r262;
	add.s64 	%rd79, %rd78, %rd8;
	shl.b64 	%rd80, %rd79, 2;
	add.s64 	%rd37, %rd4, %rd80;
	st.global.f32 	[%rd37], %f30;
	add.s64 	%rd38, %rd2, %rd77;
	ld.global.f32 	%f31, [%rd38];
	shl.b32 	%r145, %r262, 2;
	add.s32 	%r37, %r119, %r145;
	ld.shared.f32 	%f32, [%r37];
	setp.gt.f32 	%p31, %f32, %f31;
	selp.f32 	%f33, %f31, %f32, %p31;
	add.s64 	%rd39, %rd3, %rd80;
	st.global.f32 	[%rd39], %f33;
	@%p29 bra 	$L__BB0_27;

	setp.eq.s32 	%p32, %r10, 2;
	ld.global.f32 	%f34, [%rd36+4];
	ld.shared.f32 	%f35, [%r36+4];
	setp.gt.f32 	%p33, %f35, %f34;
	selp.f32 	%f36, %f35, %f34, %p33;
	st.global.f32 	[%rd37+4], %f36;
	ld.global.f32 	%f37, [%rd38+4];
	ld.shared.f32 	%f38, [%r37+4];
	setp.gt.f32 	%p34, %f38, %f37;
	selp.f32 	%f39, %f37, %f38, %p34;
	st.global.f32 	[%rd39+4], %f39;
	@%p32 bra 	$L__BB0_27;

	ld.global.f32 	%f40, [%rd36+8];
	ld.shared.f32 	%f41, [%r36+8];
	setp.gt.f32 	%p35, %f41, %f40;
	selp.f32 	%f42, %f41, %f40, %p35;
	st.global.f32 	[%rd37+8], %f42;
	ld.global.f32 	%f43, [%rd38+8];
	ld.shared.f32 	%f44, [%r37+8];
	setp.gt.f32 	%p36, %f44, %f43;
	selp.f32 	%f45, %f43, %f44, %p36;
	st.global.f32 	[%rd39+8], %f45;

$L__BB0_27:
	setp.lt.s32 	%p37, %r95, 1;
	@%p37 bra 	$L__BB0_34;

	mov.u32 	%r263, 0;

$L__BB0_29:
	mov.u32 	%r264, 0;

$L__BB0_30:
	mul.lo.s32 	%r242, %r263, %r94;
	add.s32 	%r148, %r264, %r242;
	mul.wide.s32 	%rd81, %r148, 4;
	add.s64 	%rd82, %rd18, %rd81;
	cvt.s64.s32 	%rd83, %r264;
	add.s64 	%rd40, %rd83, %rd8;
	shl.b64 	%rd84, %rd40, 2;
	add.s64 	%rd85, %rd4, %rd84;
	ld.global.f32 	%f46, [%rd85];
	ld.global.f32 	%f1, [%rd82];
	setp.gt.f32 	%p38, %f1, %f46;
	@%p38 bra 	$L__BB0_33;

	add.s64 	%rd87, %rd3, %rd84;
	ld.global.f32 	%f47, [%rd87];
	setp.lt.f32 	%p39, %f1, %f47;
	add.s32 	%r264, %r264, 1;
	@%p39 bra 	$L__BB0_33;

	setp.lt.s32 	%p40, %r264, %r94;
	@%p40 bra 	$L__BB0_30;
	bra.uni 	$L__BB0_42;

$L__BB0_33:
	add.s32 	%r263, %r263, 1;
	setp.lt.s32 	%p41, %r263, %r95;
	@%p41 bra 	$L__BB0_29;

$L__BB0_34:
	add.s32 	%r230, %r94, -1;
	setp.lt.u32 	%p91, %r230, 3;
	mov.u32 	%r267, 0;
	@%p91 bra 	$L__BB0_37;

	mov.u32 	%r267, 0;
	mov.u32 	%r266, %r11;

$L__BB0_36:
	cvt.s64.s32 	%rd88, %r267;
	add.s64 	%rd89, %rd88, %rd8;
	shl.b64 	%rd90, %rd89, 2;
	add.s64 	%rd91, %rd3, %rd90;
	ld.global.f32 	%f48, [%rd91];
	add.s32 	%r151, %r267, %r25;
	mul.wide.s32 	%rd92, %r151, 4;
	add.s64 	%rd93, %rd2, %rd92;
	st.global.f32 	[%rd93], %f48;
	add.s64 	%rd94, %rd4, %rd90;
	ld.global.f32 	%f49, [%rd94];
	add.s64 	%rd95, %rd1, %rd92;
	st.global.f32 	[%rd95], %f49;
	ld.global.f32 	%f50, [%rd91+4];
	st.global.f32 	[%rd93+4], %f50;
	ld.global.f32 	%f51, [%rd94+4];
	st.global.f32 	[%rd95+4], %f51;
	ld.global.f32 	%f52, [%rd91+8];
	st.global.f32 	[%rd93+8], %f52;
	ld.global.f32 	%f53, [%rd94+8];
	st.global.f32 	[%rd95+8], %f53;
	ld.global.f32 	%f54, [%rd91+12];
	st.global.f32 	[%rd93+12], %f54;
	ld.global.f32 	%f55, [%rd94+12];
	st.global.f32 	[%rd95+12], %f55;
	add.s32 	%r267, %r267, 4;
	add.s32 	%r266, %r266, -4;
	setp.ne.s32 	%p43, %r266, 0;
	@%p43 bra 	$L__BB0_36;

$L__BB0_37:
	setp.eq.s32 	%p92, %r10, 0;
	@%p92 bra 	$L__BB0_41;

	setp.eq.s32 	%p45, %r10, 1;
	cvt.s64.s32 	%rd96, %r267;
	add.s64 	%rd97, %rd96, %rd8;
	shl.b64 	%rd98, %rd97, 2;
	add.s64 	%rd41, %rd3, %rd98;
	ld.global.f32 	%f56, [%rd41];
	add.s32 	%r152, %r267, %r25;
	mul.wide.s32 	%rd99, %r152, 4;
	add.s64 	%rd42, %rd2, %rd99;
	st.global.f32 	[%rd42], %f56;
	add.s64 	%rd43, %rd4, %rd98;
	ld.global.f32 	%f57, [%rd43];
	add.s64 	%rd44, %rd1, %rd99;
	st.global.f32 	[%rd44], %f57;
	@%p45 bra 	$L__BB0_41;

	setp.eq.s32 	%p46, %r10, 2;
	ld.global.f32 	%f58, [%rd41+4];
	st.global.f32 	[%rd42+4], %f58;
	ld.global.f32 	%f59, [%rd43+4];
	st.global.f32 	[%rd44+4], %f59;
	@%p46 bra 	$L__BB0_41;

	ld.global.f32 	%f60, [%rd41+8];
	st.global.f32 	[%rd42+8], %f60;
	ld.global.f32 	%f61, [%rd43+8];
	st.global.f32 	[%rd44+8], %f61;

$L__BB0_41:
	cvt.s64.s32 	%rd153, %r257;
	shl.b64 	%rd152, %rd23, 2;
	add.s64 	%rd151, %rd5, %rd152;
	atom.global.min.s32 	%r153, [%rd151], -1;
	shl.b64 	%rd100, %rd153, 2;
	add.s64 	%rd101, %rd6, %rd100;
	mov.u32 	%r154, 1;
	st.global.u32 	[%rd101], %r154;

$L__BB0_42:
	add.s32 	%r257, %r257, %r97;
	setp.lt.s32 	%p47, %r257, %r96;
	@%p47 bra 	$L__BB0_18;
	bra.uni 	$L__BB0_43;

$L__BB0_12:
	mov.u32 	%r256, %r4;

$L__BB0_13:
	cvt.u32.u64 	%r132, %rd23;
	setp.eq.s32 	%p11, %r256, %r132;
	@%p11 bra 	$L__BB0_16;

	setp.gt.s32 	%p12, %r95, 0;
	cvt.s64.s32 	%rd25, %r256;
	mul.wide.s32 	%rd69, %r256, 4;
	add.s64 	%rd70, %rd5, %rd69;
	ld.global.u32 	%r133, [%rd70];
	setp.eq.s32 	%p13, %r133, -1;
	or.pred  	%p14, %p13, %p12;
	@%p14 bra 	$L__BB0_16;

	shl.b64 	%rd155, %rd23, 2;
	add.s64 	%rd154, %rd5, %rd155;
	atom.global.min.s32 	%r134, [%rd154], -1;
	shl.b64 	%rd71, %rd25, 2;
	add.s64 	%rd72, %rd6, %rd71;
	mov.u32 	%r135, 1;
	st.global.u32 	[%rd72], %r135;

$L__BB0_16:
	add.s32 	%r256, %r256, %r97;
	setp.lt.s32 	%p15, %r256, %r96;
	@%p15 bra 	$L__BB0_13;

$L__BB0_43:
	setp.ne.s64 	%p93, %rd7, 0;
	@%p93 bra 	$L__BB0_45;

	// begin inline asm
	trap;
	// end inline asm

$L__BB0_45:
	barrier.sync 	0;
	@%p7 bra 	$L__BB0_48;

	// begin inline asm
	atom.add.release.gpu.u32 %r155,[%rd11],%r7;
	// end inline asm

$L__BB0_47:
	// begin inline asm
	ld.acquire.gpu.u32 %r157,[%rd11];
	// end inline asm
	xor.b32  	%r158, %r157, %r155;
	setp.gt.s32 	%p50, %r158, -1;
	@%p50 bra 	$L__BB0_47;

$L__BB0_48:
	setp.ge.s32 	%p94, %r4, %r96;
	barrier.sync 	0;
	@%p94 bra 	$L__BB0_70;

	mov.u32 	%r268, 0;
	mov.u32 	%r269, %r4;

$L__BB0_50:
	not.b32 	%r247, %r254;
	add.s32 	%r246, %r4, %r247;
	mad.lo.s32 	%r54, %r268, %r97, %r246;
	setp.le.s32 	%p52, %r269, %r254;
	@%p52 bra 	$L__BB0_69;

	mul.wide.s32 	%rd104, %r269, 4;
	add.s64 	%rd45, %rd157, %rd104;
	ld.global.u32 	%r55, [%rd45];
	mul.wide.s32 	%rd105, %r55, 4;
	add.s64 	%rd106, %rd6, %rd105;
	ld.global.u32 	%r161, [%rd106];
	setp.eq.s32 	%p53, %r161, 1;
	@%p53 bra 	$L__BB0_60;
	bra.uni 	$L__BB0_52;

$L__BB0_60:
	add.s32 	%r251, %r269, -1;
	setp.le.s32 	%p67, %r251, %r254;
	mov.u32 	%r282, 0;
	@%p67 bra 	$L__BB0_68;

	add.s32 	%r280, %r269, -1;
	and.b32  	%r72, %r54, 3;
	add.s32 	%r191, %r54, -1;
	setp.lt.u32 	%p68, %r191, 3;
	mov.u32 	%r282, 0;
	@%p68 bra 	$L__BB0_64;

	add.s32 	%r280, %r269, -1;
	sub.s32 	%r278, %r54, %r72;
	mov.u32 	%r282, 0;

$L__BB0_63:
	mul.wide.s32 	%rd125, %r280, 4;
	add.s64 	%rd126, %rd157, %rd125;
	ld.global.u32 	%r193, [%rd126];
	mul.wide.s32 	%rd127, %r193, 4;
	add.s64 	%rd128, %rd6, %rd127;
	ld.global.u32 	%r194, [%rd128];
	setp.eq.s32 	%p69, %r194, 1;
	selp.u32 	%r195, 1, 0, %p69;
	add.s32 	%r196, %r282, %r195;
	ld.global.u32 	%r197, [%rd126+-4];
	mul.wide.s32 	%rd129, %r197, 4;
	add.s64 	%rd130, %rd6, %rd129;
	ld.global.u32 	%r198, [%rd130];
	setp.eq.s32 	%p70, %r198, 1;
	selp.u32 	%r199, 1, 0, %p70;
	add.s32 	%r200, %r196, %r199;
	ld.global.u32 	%r201, [%rd126+-8];
	mul.wide.s32 	%rd131, %r201, 4;
	add.s64 	%rd132, %rd6, %rd131;
	ld.global.u32 	%r202, [%rd132];
	setp.eq.s32 	%p71, %r202, 1;
	selp.u32 	%r203, 1, 0, %p71;
	add.s32 	%r204, %r200, %r203;
	ld.global.u32 	%r205, [%rd126+-12];
	mul.wide.s32 	%rd133, %r205, 4;
	add.s64 	%rd134, %rd6, %rd133;
	ld.global.u32 	%r206, [%rd134];
	setp.eq.s32 	%p72, %r206, 1;
	selp.u32 	%r207, 1, 0, %p72;
	add.s32 	%r282, %r204, %r207;
	add.s32 	%r280, %r280, -4;
	add.s32 	%r278, %r278, -4;
	setp.ne.s32 	%p73, %r278, 0;
	@%p73 bra 	$L__BB0_63;

$L__BB0_64:
	setp.eq.s32 	%p74, %r72, 0;
	@%p74 bra 	$L__BB0_68;

	mul.wide.s32 	%rd135, %r280, 4;
	add.s64 	%rd46, %rd157, %rd135;
	ld.global.u32 	%r208, [%rd46];
	mul.wide.s32 	%rd136, %r208, 4;
	add.s64 	%rd137, %rd6, %rd136;
	ld.global.u32 	%r209, [%rd137];
	setp.eq.s32 	%p75, %r209, 1;
	selp.u32 	%r210, 1, 0, %p75;
	add.s32 	%r282, %r282, %r210;
	setp.eq.s32 	%p76, %r72, 1;
	@%p76 bra 	$L__BB0_68;

	ld.global.u32 	%r211, [%rd46+-4];
	mul.wide.s32 	%rd138, %r211, 4;
	add.s64 	%rd139, %rd6, %rd138;
	ld.global.u32 	%r212, [%rd139];
	setp.eq.s32 	%p77, %r212, 1;
	selp.u32 	%r213, 1, 0, %p77;
	add.s32 	%r282, %r282, %r213;
	setp.eq.s32 	%p78, %r72, 2;
	@%p78 bra 	$L__BB0_68;

	ld.global.u32 	%r214, [%rd46+-8];
	mul.wide.s32 	%rd140, %r214, 4;
	add.s64 	%rd141, %rd6, %rd140;
	ld.global.u32 	%r215, [%rd141];
	setp.eq.s32 	%p79, %r215, 1;
	selp.u32 	%r216, 1, 0, %p79;
	add.s32 	%r282, %r282, %r216;

$L__BB0_68:
	not.b32 	%r217, %r282;
	add.s32 	%r218, %r217, %r96;
	mul.wide.s32 	%rd142, %r218, 4;
	add.s64 	%rd143, %rd158, %rd142;
	st.global.u32 	[%rd143], %r55;
	bra.uni 	$L__BB0_69;

$L__BB0_52:
	add.s32 	%r275, %r254, 1;
	add.s32 	%r244, %r269, -1;
	setp.le.s32 	%p54, %r244, %r254;
	@%p54 bra 	$L__BB0_59;

	add.s32 	%r275, %r254, 1;
	add.s32 	%r270, %r269, -1;
	and.b32  	%r57, %r54, 3;
	setp.eq.s32 	%p55, %r57, 0;
	@%p55 bra 	$L__BB0_57;

	add.s32 	%r250, %r254, 1;
	ld.global.u32 	%r163, [%rd45+-4];
	mul.wide.s32 	%rd107, %r163, 4;
	add.s64 	%rd108, %rd6, %rd107;
	ld.global.u32 	%r164, [%rd108];
	setp.eq.s32 	%p56, %r164, 0;
	selp.u32 	%r165, 1, 0, %p56;
	add.s32 	%r275, %r250, %r165;
	add.s32 	%r270, %r269, -2;
	setp.eq.s32 	%p57, %r57, 1;
	@%p57 bra 	$L__BB0_57;

	ld.global.u32 	%r166, [%rd45+-8];
	mul.wide.s32 	%rd109, %r166, 4;
	add.s64 	%rd110, %rd6, %rd109;
	ld.global.u32 	%r167, [%rd110];
	setp.eq.s32 	%p58, %r167, 0;
	selp.u32 	%r168, 1, 0, %p58;
	add.s32 	%r275, %r275, %r168;
	add.s32 	%r270, %r269, -3;
	setp.eq.s32 	%p59, %r57, 2;
	@%p59 bra 	$L__BB0_57;

	ld.global.u32 	%r169, [%rd45+-12];
	mul.wide.s32 	%rd111, %r169, 4;
	add.s64 	%rd112, %rd6, %rd111;
	ld.global.u32 	%r170, [%rd112];
	setp.eq.s32 	%p60, %r170, 0;
	selp.u32 	%r171, 1, 0, %p60;
	add.s32 	%r275, %r275, %r171;
	add.s32 	%r270, %r269, -4;

$L__BB0_57:
	add.s32 	%r172, %r54, -1;
	setp.lt.u32 	%p61, %r172, 3;
	@%p61 bra 	$L__BB0_59;

$L__BB0_58:
	mul.wide.s32 	%rd113, %r270, 4;
	add.s64 	%rd114, %rd157, %rd113;
	ld.global.u32 	%r173, [%rd114];
	mul.wide.s32 	%rd115, %r173, 4;
	add.s64 	%rd116, %rd6, %rd115;
	ld.global.u32 	%r174, [%rd116];
	setp.eq.s32 	%p62, %r174, 0;
	selp.u32 	%r175, 1, 0, %p62;
	add.s32 	%r176, %r275, %r175;
	ld.global.u32 	%r177, [%rd114+-4];
	mul.wide.s32 	%rd117, %r177, 4;
	add.s64 	%rd118, %rd6, %rd117;
	ld.global.u32 	%r178, [%rd118];
	setp.eq.s32 	%p63, %r178, 0;
	selp.u32 	%r179, 1, 0, %p63;
	add.s32 	%r180, %r176, %r179;
	ld.global.u32 	%r181, [%rd114+-8];
	mul.wide.s32 	%rd119, %r181, 4;
	add.s64 	%rd120, %rd6, %rd119;
	ld.global.u32 	%r182, [%rd120];
	setp.eq.s32 	%p64, %r182, 0;
	selp.u32 	%r183, 1, 0, %p64;
	add.s32 	%r184, %r180, %r183;
	ld.global.u32 	%r185, [%rd114+-12];
	mul.wide.s32 	%rd121, %r185, 4;
	add.s64 	%rd122, %rd6, %rd121;
	ld.global.u32 	%r186, [%rd122];
	setp.eq.s32 	%p65, %r186, 0;
	selp.u32 	%r187, 1, 0, %p65;
	add.s32 	%r275, %r184, %r187;
	add.s32 	%r270, %r270, -4;
	setp.gt.s32 	%p66, %r270, %r254;
	@%p66 bra 	$L__BB0_58;

$L__BB0_59:
	mul.wide.s32 	%rd123, %r275, 4;
	add.s64 	%rd124, %rd158, %rd123;
	st.global.u32 	[%rd124], %r55;

$L__BB0_69:
	add.s32 	%r269, %r269, %r97;
	setp.lt.s32 	%p80, %r269, %r96;
	add.s32 	%r268, %r268, 1;
	@%p80 bra 	$L__BB0_50;

$L__BB0_70:
	setp.ne.s64 	%p95, %rd7, 0;
	@%p95 bra 	$L__BB0_72;

	// begin inline asm
	trap;
	// end inline asm

$L__BB0_72:
	barrier.sync 	0;
	@%p7 bra 	$L__BB0_75;

	// begin inline asm
	atom.add.release.gpu.u32 %r219,[%rd11],%r7;
	// end inline asm

$L__BB0_74:
	// begin inline asm
	ld.acquire.gpu.u32 %r221,[%rd11];
	// end inline asm
	xor.b32  	%r222, %r221, %r219;
	setp.gt.s32 	%p83, %r222, -1;
	@%p83 bra 	$L__BB0_74;

$L__BB0_75:
	setp.ge.s32 	%p96, %r4, %r96;
	barrier.sync 	0;
	@%p96 bra 	$L__BB0_82;

	setp.eq.s32 	%p85, %r12, 0;
	mov.u32 	%r283, %r4;
	@%p85 bra 	$L__BB0_80;

	add.s32 	%r283, %r4, %r97;
	setp.eq.s32 	%p86, %r12, 1;
	mov.u32 	%r223, 0;
	st.global.u32 	[%rd12], %r223;
	@%p86 bra 	$L__BB0_80;

	add.s32 	%r239, %r4, %r97;
	add.s32 	%r283, %r239, %r97;
	setp.eq.s32 	%p87, %r12, 2;
	st.global.u32 	[%rd13], %r223;
	@%p87 bra 	$L__BB0_80;

	mov.u32 	%r225, 0;
	st.global.u32 	[%rd14], %r225;
	mov.u32 	%r283, %r15;

$L__BB0_80:
	setp.lt.u32 	%p88, %r9, 3;
	@%p88 bra 	$L__BB0_82;

$L__BB0_81:
	mul.wide.s32 	%rd146, %r283, 4;
	add.s64 	%rd147, %rd6, %rd146;
	mov.u32 	%r226, 0;
	st.global.u32 	[%rd147], %r226;
	add.s64 	%rd148, %rd147, %rd17;
	st.global.u32 	[%rd148], %r226;
	add.s32 	%r227, %r283, %r97;
	add.s32 	%r228, %r227, %r97;
	add.s64 	%rd149, %rd148, %rd17;
	st.global.u32 	[%rd149], %r226;
	add.s32 	%r229, %r228, %r97;
	add.s64 	%rd150, %rd149, %rd17;
	st.global.u32 	[%rd150], %r226;
	add.s32 	%r283, %r229, %r97;
	setp.lt.s32 	%p89, %r283, %r96;
	@%p89 bra 	$L__BB0_81;

$L__BB0_82:
	add.s32 	%r254, %r254, 1;
	setp.lt.s32 	%p90, %r254, %r96;
	@%p90 bra 	$L__BB0_2;

$L__BB0_83:
	ret;

}

